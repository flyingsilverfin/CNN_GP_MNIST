\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{krizhevsky2012imagenet}
\citation{szegedy2013intriguing}
\citation{bridle1990probabilistic}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\newlabel{sec:background}{{2}{1}{}{section.2}{}}
\newlabel{sec:background:cnn}{{2.1}{1}{}{subsection.2.1}{}}
\newlabel{sec:background:gp}{{2.2}{1}{}{subsection.2.2}{}}
\citation{hensman2015scalable}
\citation{rasmussen2006gaussian}
\citation{goodfellow2014explaining}
\citation{papernot2016limitations}
\citation{paass1993assessing}
\citation{mackay1992practical}
\citation{blundell2015weight}
\citation{gal2016dropout}
\citation{Bradshaw2017}
\citation{samangouei2018defense}
\citation{zantedeschi2017efficient}
\citation{chollet2015keras}
\citation{GPflow2017}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gp:basic}{{1}{2}{A simple 1-D GP regression. The dashed lines represent $\pm 2\sigma $ from the mean, and blue points are training data. Notice how the variance increases away from known datapoints.\relax }{figure.caption.1}{}}
\newlabel{sec:background:adv}{{2.3}{2}{}{subsection.2.3}{}}
\citation{rasmussen2006gaussian}
\newlabel{sec:mnist}{{5}{3}{}{section.5}{}}
\newlabel{sec:mnist:acc}{{5.1}{3}{}{subsection.5.1}{}}
\newlabel{tab:model_accuracies}{{1}{3}{MNIST test set accuracy of of various models. The Hybrid model will be developed in Section~\ref {sec:mnist:hybridization}.\relax }{table.caption.2}{}}
\newlabel{fig:mnist-variances}{{2}{3}{Distribution of Variances for both incorrect and correctly classified examples.\relax }{figure.caption.3}{}}
\newlabel{tab:statsig}{{2}{3}{Statistically Significant Predictions on MNIST using 95\% confidence for the \textit {combined} kernel.\relax }{table.caption.4}{}}
\newlabel{sec:svgp-variance}{{5.4}{4}{}{subsection.5.4}{}}
\newlabel{fig:all_variances}{{3}{4}{Plotting class probability versus variance on the standard MNIST dataset. They are totally correlated and thus one completely determines the other.\relax }{figure.caption.5}{}}
\newlabel{tab:misprediction}{{3}{4}{Mean and Standard Deviation of probability of incorrect predicted classes for the \textit {combined} kernel GP and the CNN.\relax }{table.caption.6}{}}
\newlabel{tab:mispredictions}{{3}{4}{Mean and Standard Deviation of probability of incorrect predicted classes for the \textit {combined} kernel GP and the CNN.\relax }{table.caption.6}{}}
\newlabel{fig:venn}{{4}{4}{Overlap in misclassifications of the CNN and the \textit {combined} GP model.\relax }{figure.caption.7}{}}
\newlabel{sec:mnist:hybridization}{{5.6}{4}{}{subsection.5.6}{}}
\citation{basu2017learning}
\newlabel{fig:difficult}{{5}{5}{An example of a truly difficult image that both models fail on. The green bar represents the true class, while the tallest red one is the predicted class.\relax }{figure.caption.8}{}}
\newlabel{sec:mnist:hybridization:criteria}{{5.6.1}{5}{}{subsubsection.5.6.1}{}}
\newlabel{fig:mnist:mismatch}{{6}{5}{Examining cases of misclassifications. The left column corresponds to one of the 16 examples the CNN classified incorrectly, while the right column stems from the GP's incorrect classifications. With the right heuristic these case, and others, may be salvageable.\relax }{figure.caption.9}{}}
\citation{papernot2017cleverhans}
\newlabel{tab:nmnist:accuracies}{{4}{6}{Accuracy across all models and n-MNIST data sets.\relax }{table.caption.10}{}}
\newlabel{fig:nmist:vars}{{8}{6}{Variances across the n-MNIST dataset\relax }{figure.caption.12}{}}
\newlabel{sec:adversarial}{{7}{6}{}{section.7}{}}
\citation{Bradshaw2017}
\newlabel{tab:adv:accuracies}{{5}{7}{Mean and Standard Deviation of probability of incorrect predicted classes for the \textit {combined} kernel GP and the CNN.\relax }{table.caption.13}{}}
\newlabel{tab:adv_statsig}{{6}{7}{Statistically Significant Predictions on Perturbed MNIST using 95\% confidence.\relax }{table.caption.14}{}}
\newlabel{fig:adv:epsilons}{{9}{7}{Classification Accuracies on FGSM attacked MNIST images across varying $\epsilon $\relax }{figure.caption.15}{}}
\newlabel{sec:discussion}{{8}{7}{}{section.8}{}}
\citation{scheirer2013toward}
\citation{abdessalem2017automatic}
\citation{duvenaud2014automatic}
\bibdata{gp_cnn_bibliography.bib}
\bibcite{abdessalem2017automatic}{{1}{2017}{{Abdessalem et~al.}}{{Abdessalem, Dervilis, Wagg, and Worden}}}
\bibcite{basu2017learning}{{2}{2017}{{Basu et~al.}}{{Basu, Karki, Ganguly, DiBiano, Mukhopadhyay, Gayaka, Kannan, and Nemani}}}
\bibcite{blundell2015weight}{{3}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{Bradshaw2017}{{4}{2017}{{Bradshaw et~al.}}{{Bradshaw, Matthews, and Ghahramani}}}
\bibcite{bridle1990probabilistic}{{5}{1990}{{Bridle}}{{}}}
\bibcite{chollet2015keras}{{6}{2015}{{Chollet et~al.}}{{}}}
\bibcite{duvenaud2014automatic}{{7}{2014}{{Duvenaud}}{{}}}
\bibcite{gal2016dropout}{{8}{2016}{{Gal \& Ghahramani}}{{Gal and Ghahramani}}}
\bibcite{goodfellow2014explaining}{{9}{2014}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{hensman2015scalable}{{10}{2015}{{Hensman et~al.}}{{Hensman, Matthews, and Ghahramani}}}
\bibcite{krizhevsky2012imagenet}{{11}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lecun1998gradient}{{12}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{mackay1992practical}{{13}{1992}{{MacKay~David}}{{}}}
\bibcite{GPflow2017}{{14}{2017}{{Matthews et~al.}}{{Matthews, {van der Wilk}, Nickson, Fujii, {Boukouvalas}, {Le{\'o}n-Villagr{\'a}}, Ghahramani, and Hensman}}}
\bibcite{papernot2017cleverhans}{{15}{2017}{{Nicolas~Papernot}}{{}}}
\bibcite{paass1993assessing}{{16}{1993}{{Paass}}{{}}}
\bibcite{papernot2016limitations}{{17}{2016}{{Papernot et~al.}}{{Papernot, McDaniel, Jha, Fredrikson, Celik, and Swami}}}
\bibcite{rasmussen2006gaussian}{{18}{2006}{{Rasmussen \& Williams}}{{Rasmussen and Williams}}}
\bibcite{samangouei2018defense}{{19}{2018}{{Samangouei et~al.}}{{Samangouei, Kabkab, and Chellappa}}}
\bibcite{scheirer2013toward}{{20}{2013}{{Scheirer et~al.}}{{Scheirer, de~Rezende~Rocha, Sapkota, and Boult}}}
\bibcite{szegedy2013intriguing}{{21}{2013}{{Szegedy et~al.}}{{Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus}}}
\bibcite{zantedeschi2017efficient}{{22}{2017}{{Zantedeschi et~al.}}{{Zantedeschi, Nicolae, and Rawat}}}
\bibstyle{icml2018}
