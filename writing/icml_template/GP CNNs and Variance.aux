\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{krizhevsky2012imagenet}
\citation{szegedy2013intriguing}
\citation{lecun1998gradient}
\citation{basu2017learning}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\newlabel{sec:background}{{2}{1}{}{section.2}{}}
\newlabel{sec:background:cnn}{{2.1}{1}{}{subsection.2.1}{}}
\citation{bridle1990probabilistic}
\citation{hensman2015scalable}
\citation{rasmussen2006gaussian}
\citation{goodfellow2014explaining}
\citation{papernot2016limitations}
\citation{mackay1992practical}
\citation{blundell2015weight}
\citation{zhu2017deep}
\citation{gal2016dropout}
\citation{Bradshaw2017}
\newlabel{sec:background:gp}{{2.2}{2}{}{subsection.2.2}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gp:basic}{{1}{2}{A simple one-dimensional GP regression. The dashed lines represent two standard deviations from the mean, and blue points are training data. Notice how the variance increases away from known datapoints.\relax }{figure.caption.1}{}}
\newlabel{sec:background:adv}{{2.3}{2}{}{subsection.2.3}{}}
\citation{samangouei2018defense}
\citation{zantedeschi2017efficient}
\citation{chollet2015keras}
\citation{GPflow2017}
\citation{rasmussen2006gaussian}
\newlabel{sec:mnist}{{5}{3}{}{section.5}{}}
\newlabel{sec:mnist:acc}{{5.1}{3}{}{subsection.5.1}{}}
\newlabel{tab:model_accuracies}{{1}{3}{MNIST test set accuracy of of various models. The Hybrid model will be developed in Section~\ref {sec:mnist:hybridization}.\relax }{table.caption.2}{}}
\newlabel{fig:mnist-variances}{{2}{4}{Distribution of Variances across correctly classified examples. 95\% of these classifications occur with a variance of 0.02 or less.\relax }{figure.caption.3}{}}
\newlabel{sec:svgp-variance}{{5.3}{4}{}{subsection.5.3}{}}
\newlabel{fig:all_variances}{{3}{4}{Plotting class probability versus variance on the standard MNIST dataset. They are totally correlated and thus one completely determines the other.\relax }{figure.caption.4}{}}
\newlabel{tab:misprediction}{{2}{4}{Mean and Standard Deviation of probability of incorrect predicted classes for the \textit {combined} kernel GP and the CNN.\relax }{table.caption.5}{}}
\newlabel{tab:mispredictions}{{2}{4}{Mean and Standard Deviation of probability of incorrect predicted classes for the \textit {combined} kernel GP and the CNN.\relax }{table.caption.5}{}}
\newlabel{fig:venn}{{4}{4}{Overlap in misclassifications of the CNN and the \textit {combined} GP model.\relax }{figure.caption.6}{}}
\newlabel{fig:difficult}{{5}{5}{An example of a truly difficult image that both models fail on. The green bar represents the true class, while the tallest red one is the predicted class.\relax }{figure.caption.7}{}}
\newlabel{sec:mnist:hybridization}{{5.5}{5}{}{subsection.5.5}{}}
\newlabel{sec:mnist:hybridization:criteria}{{5.5.1}{5}{}{subsubsection.5.5.1}{}}
\newlabel{fig:mnist:mismatch}{{6}{5}{Examining cases of misclassifications. The left column corresponds to one of the 16 examples the CNN classified incorrectly, while the right column stems from the GP's incorrect classifications. With the right heuristic these case, and others, may be salvageable.\relax }{figure.caption.8}{}}
\citation{basu2017learning}
\newlabel{fig:nmist:vars}{{8}{6}{Variances across the n-MNIST dataset\relax }{figure.caption.11}{}}
\citation{Bradshaw2017}
\newlabel{tab:adv:accuracies}{{3}{7}{Performance across all models and n-MNIST data sets.\relax }{table.caption.10}{}}
\newlabel{sec:adversarial}{{7}{7}{}{section.7}{}}
\newlabel{tab:adv:accuracies}{{4}{7}{Mean and Standard Deviation of probability of incorrect predicted classes for the \textit {combined} kernel GP and the CNN.\relax }{table.caption.12}{}}
\newlabel{fig:adv:epsilons}{{9}{7}{Classification Accuracies on FGSM attacked MNIST images across varying $\epsilon $\relax }{figure.caption.13}{}}
\newlabel{sec:discussion}{{8}{7}{}{section.8}{}}
\citation{abdessalem2017automatic}
\citation{duvenaud2014automatic}
\bibdata{gp_cnn_bibliography.bib}
\bibcite{abdessalem2017automatic}{{1}{2017}{{Abdessalem et~al.}}{{Abdessalem, Dervilis, Wagg, and Worden}}}
\bibcite{basu2017learning}{{2}{2017}{{Basu et~al.}}{{Basu, Karki, Ganguly, DiBiano, Mukhopadhyay, Gayaka, Kannan, and Nemani}}}
\bibcite{blundell2015weight}{{3}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{Bradshaw2017}{{4}{2017}{{Bradshaw et~al.}}{{Bradshaw, Matthews, and Ghahramani}}}
\bibcite{bridle1990probabilistic}{{5}{1990}{{Bridle}}{{}}}
\bibcite{chollet2015keras}{{6}{2015}{{Chollet et~al.}}{{}}}
\bibcite{duvenaud2014automatic}{{7}{2014}{{Duvenaud}}{{}}}
\bibcite{gal2016dropout}{{8}{2016}{{Gal \& Ghahramani}}{{Gal and Ghahramani}}}
\bibcite{goodfellow2014explaining}{{9}{2014}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{hensman2015scalable}{{10}{2015}{{Hensman et~al.}}{{Hensman, Matthews, and Ghahramani}}}
\bibcite{krizhevsky2012imagenet}{{11}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lecun1998gradient}{{12}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{mackay1992practical}{{13}{1992}{{MacKay~David}}{{}}}
\bibcite{GPflow2017}{{14}{2017}{{Matthews et~al.}}{{Matthews, {van der Wilk}, Nickson, Fujii, {Boukouvalas}, {Le{\'o}n-Villagr{\'a}}, Ghahramani, and Hensman}}}
\bibcite{papernot2016limitations}{{15}{2016}{{Papernot et~al.}}{{Papernot, McDaniel, Jha, Fredrikson, Celik, and Swami}}}
\bibcite{rasmussen2006gaussian}{{16}{2006}{{Rasmussen \& Williams}}{{Rasmussen and Williams}}}
\bibcite{samangouei2018defense}{{17}{2018}{{Samangouei et~al.}}{{Samangouei, Kabkab, and Chellappa}}}
\bibcite{szegedy2013intriguing}{{18}{2013}{{Szegedy et~al.}}{{Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus}}}
\bibcite{zantedeschi2017efficient}{{19}{2017}{{Zantedeschi et~al.}}{{Zantedeschi, Nicolae, and Rawat}}}
\bibcite{zhu2017deep}{{20}{2017}{{Zhu \& Laptev}}{{Zhu and Laptev}}}
\bibstyle{icml2018}
