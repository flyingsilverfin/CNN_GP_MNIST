%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2018}


% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2018}

\begin{document}

\twocolumn[
\icmltitle{Gaussian Processes and CNNs: The utility of Prediction Variance}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.


\begin{icmlauthorlist}
\icmlauthor{Joshua Send}{to}
\end{icmlauthorlist}

\icmlaffiliation{to}{University of Cambridge}

\icmlcorrespondingauthor{Joshua Send}{js2173@cam.ac.uk}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}

4-6 sentences
TODO

\end{abstract}

\section{Introduction}
\label{sec:intro}


* Introductory paragraph
 * Problem with CNNs predictions
 * How GPs are bayesian predictors that give variance
 * This work combines ... by ... showing ... (or something)
 * "Interpretable ML"
 
CNNs
 * Quick introduction to CNNs
 * How probabilities are calculated (Classification, softmax, alternatives discuss briefly)
 * Use as a feature extractor
 * 

GPs
 * Quick introduction GPs
 * Bayesian, provide variance
 * Hopefully have higher variance when away from the space they are trained in
   * show 2D image of a fitted GP with variances for intuition
 * Covariance function/kernel steers behavior, different options are evaluated, can combine kernels but not some others
 * Classification versus regression
  
In general, how we can combine these 
 * Using CNN as feature extractor, replacing Softmax with GP
 * What this paper will explore
 * MNIST, N-MNIST (\cite{basu2017learning}), Adverserial examples
 * Comparing CNN, GP with 2 different Kernels, two Hybrid models (ref to section where this decision is justified)
 



\section{Related Work}

* Variance from CNNs
* GPDNN
* Adverserial attacks, NN robustness to new examples (! TODO reading)

 
\section{Implementation}
* GPFlow, alternatives explored
 * Batching, different GP mechanisms to deal with O(N\^3) scalability
 * Inducing points
 * Approximate trainining time for GP (=> discussion?)
 * Predict time for GP (=> discussion?)

* Keras MNIST
 * 

* Train, test sizes across MNIST, NMNIST, Adverserial, and image sizes (28x28, grayscale)
* Balanced datasets? (! TODO)
* OPTIONAL: check performance across specific numbers?


* Hybrid model (=> own Section?)

\section{MNIST}

\subsection{Accuracy of Various Models}
* CNN performance
* Show GP performance across various kernels
* Many configurations were explored, results
 * White noise variance is not a big factor in performance
 * Performance is most correlated with 
* Explain decision to use Matern12 and Linear*Matern32
 * inherits linear robustness to Blur/Low contrast
 * Inherits nice Matern32 properties for Hybridization
 * not that roboust to adverserial... while Matern12 is

* footnote SVM as a example of another kernel method

\subsection{Distribution of Variance}
* Show CDF of incorrect, correct predictions
* Discuss how this is useful

\subsection{GP Variance}
* Plot GP prediction probability versus confidence
* Show it's fully deterministic (TODO double check code to make sure it is)
* Conclude variance is not actually extra information but useful for interpretation
* Discuss interpretability, show some plots

\subsection{Examining Misclassifications}
* Plot CNN and GP mis-prediction probabilities
 * Discuss... conclusion is that CNN more confidently mis-predicts results?
* Show examples that both fail, one fails
* Discuss that nothing can be done when both fail - no extra information
* Show overlap, non-overlap in misclassifications
* Inspires Hybridization!

\subsection{Hybridization}
* Might be able to rescue these individual misclassifications
 * steered by variance!
 * Note that we give up interpretability here for higher accuracy in some cases, but we can notify the users the uncertainty is higher because the models disagree!	
 * some kernels better than other

* Describe both criteria, pros and cons of each
* show some results with different criteria for some example where there's a significant difference between CNN and GP (Low contrast Linear*Matern32?)
* Will show results from 'stronger@0.5' with Linear*Matern32 which performs best of all tested kernels at hybridization due to having the least overlapping misclassifications


Gains from Hybridization are usually not present

\section{N-MNIST}

\subsection{White Noise + MNIST}
 (Sample image) 
 
* Accuracy across, CNN, Matern12, Poly, Linear*Matern32, Hybridized
* Distribution of correct, incorrect classification Variances for Linear*Matern32

\subsection{Blurred MNIST}
 (Sample image)
* Accuracy across, CNN, Matern12, Poly, Linear*Matern32, Hybridized
* Distribution of correct, incorrect classification Variances for Linear*Matern32

\subsection{White Noise and Low Contrast MNIST}
 (Sample image)
* Accuracy across, CNN, Matern12, Poly, Linear*Matern32, Hybridized
* Distribution of correct, incorrect classification Variances for Linear*Matern32



\section{Adverserial Attacks}
* Brief description of FSGM, that it uses the trained CNN to generate adverserial examples with some epsilon

* Accuracy across models as epsilon varies

* Distribution of correct, incorrect classification Variances for Linear*Matern32 for eps=0.2


Citations within the text should include the authors' last names and
year. If the authors' names are included in the sentence, place only
the year in parentheses, for example when referencing Arthur Samuel's
pioneering work \yrcite{Samuel59}. Otherwise place the entire
reference in parentheses with the authors and year separated by a
comma \cite{Samuel59}. List multiple references separated by
semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
construct only for citations with three or more authors or after
listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

Authors should cite their own work in the third person
in the initial version of their paper submitted for blind review.
Please refer to Section~\ref{author info} for detailed instructions on how to
cite your own papers.

Use an unnumbered first-level section heading for the references, and use a
hanging indent style, with the first line of the reference flush against the
left margin and subsequent lines indented by 10 points. The references at the
end of this document give examples for journal articles \cite{Samuel59},
conference publications \cite{langley00}, book chapters \cite{Newell81}, books
\cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
\cite{mitchell80}, and dissertations \cite{kearns89}.

Alphabetize references by the surnames of the first authors, with
single author entries preceding multiple author entries. Order
references for the same authors by year of publication, with the
earliest first. Make sure that each reference includes all relevant
information (e.g., page numbers).


\subsection{Software and Data}

Code, results, and this report can be found at:

\url{https://github.com/flyingsilverfin/CNN_GP_MNIST}

Please note that intermediate results are not saved but can be recomputed, and that some of the configurations are specific to the development machine. % TODO update README on GitHub

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

\textbf{Do not} include acknowledgements in the initial version of
the paper submitted for blind review.

If a paper is accepted, the final camera-ready version can (and
probably should) include acknowledgements. In this case, please
place such acknowledgements in an unnumbered section at the
end of the paper. Typically, this will include thanks to reviewers
who gave useful comments, to colleagues who contributed to the ideas,
and to funding agencies and corporate sponsors that provided financial
support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{gp_cnn_bibliography.bib}
\bibliographystyle{icml2018}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Do \emph{not} have an appendix here}

\textbf{\emph{Do not put content after the references.}}
%
Put anything that you might normally include after the references in a separate
supplementary file.

We recommend that you build supplementary material in a separate document.
If you must create one PDF and cut it up, please be careful to use a tool that
doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
pdftk usually works fine. 
	
\textbf{Please do not use Apple's preview to cut off supplementary material.} In
previous years it has altered margins, and created headaches at the camera-ready
stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
